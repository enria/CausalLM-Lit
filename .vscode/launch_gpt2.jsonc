{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "News train(gpt2)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal", 
            "env": {"CUDA_VISIBLE_DEVICES":"4"},
            "cwd": "${workspaceFolder}/datas/",
            "justMyCode": true,
            "args": [
                "--warmup_rate","0",
                "--batch_size","16",
                "--model_config", "gpt2",
                "--data_config", "news"
            ],
            "presentation": {"group": "News"}
        },
        {
            "name": "VQAv2 train (llama)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "env": {"CUDA_VISIBLE_DEVICES":"4"},
            "justMyCode": false,
            "args": [
                "--stage","train",
                "--batch_size","16",
                "--ckpt_save_path", "weights/vqav2",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqav2",
                "--project", "VQAv2",
                "--run_name", "llama"
            ],
            "presentation": { "group": "VQAv2"}
        },
        {
            "name": "VQAv2 test (llama)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "env": {"CUDA_VISIBLE_DEVICES":"4"},
            "justMyCode": false,
            "args": [
                "--stage","test",
                "--batch_size","16",
                "--ckpt_save_path", "weights/vqav2",
                "--resume_ckpt", "base_acc=52.929_epoch=1.ckpt",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqav2",
                "--project", "VQAv2",
                "--run_name", "llama"
            ],
            "presentation": { "group": "VQAv2"}
        },
        {
            "name": "VQA Merge datacheck",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "env": {"CUDA_VISIBLE_DEVICES":"0"},
            "justMyCode": false,
            "args": [
                "--stage","data",
                "--batch_size","1",
                "--model_config", "gpt2",
                "--data_config", "vqa_merge"
            ],
            "presentation": { "group": "VQA-Merge"}
        },
        {
            "name": "VQA Merge train(llama-bf16)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--warmup_rate","0",
                "--batch_size","4",
                "--model_config", "llama-7b-bf16",
                "--data_config", "vqa_merge",
                "--num_beams","1",
                "--ckpt_save_path", "weights/vqa_merge",
                "--project","VQA Merge",
                "--run_name","lightning_llama-7b-bf16_3type"
            ],
            "presentation": { "group": "VQA-Merge"}
        },
        {
            "name": "VQA Merge predict(llama-bf16)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--stage","predict",
                "--batch_size","8",
                "--model_config", "llama-7b-bf16",
                "--data_config", "vqa_merge",
                "--num_beams","1",
                "--ckpt_save_path", "weights/vqa_merge",
                "--predict_ckpt_name", "base_f1=51.903_epoch=5.ckpt"
            ],
            "presentation": { "group": "VQA-Merge"}
        },
        {
            "name": "VQA Merge train(llama)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--warmup_rate","0",
                "--batch_size","4",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqa_merge",
                "--num_beams","1",
                "--ckpt_save_path", "weights/vqa_merge",
                // "--project","VQA Merge",
                // "--run_name","lightning_llama-7b-bf16_3type",
                "--M.peft.r=64"
            ],
            "presentation": { "group": "VQA-Merge"}
        },
        {
            "name": "VQA Merge test(llama)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "env": {"CUDA_VISIBLE_DEVICES":"0"},
            "justMyCode": false,
            "args": [
                "--stage","test",
                "--batch_size","4",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqa_merge",
                "--num_beams","1",
                "--ckpt_save_path", "weights/vqa_merge/lt_peft",
                "--resume_ckpt", "base_f1=50.865_epoch=5.ckpt"
            ],
            "presentation": { "group": "VQA-Merge"}
        },
        {
            "name": "VQA Merge predict(llama)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--stage","predict",
                "--batch_size","8",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqa_merge",
                "--num_beams","1",
                "--ckpt_save_path", "weights/vqa_merge",
                "--predict_ckpt_name", "base_f1=50.865_epoch=6.ckpt"
            ],
            "presentation": { "group": "VQA-Merge"}
        },
        {
            "name": "STaR4VQA train(gpt2)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--stage","train",
                "--max_epochs", "20",
                "--batch_size","16",
                "--model_config", "gpt2",
                "--data_config", "instruction",
                "--num_beams","1",
                "--ckpt_save_path","/home/yadong/workspace/llama/STaR4VQA/output/bootstrap/epoch0/out",
                "--D.datamodule.args.data_dir=/home/yadong/workspace/llama/STaR4VQA/output/bootstrap/epoch0",
                "--D.datamodule.args.train_name=instruction.json"
            ],
            "presentation": { "group": "STaR4VQA" }
        },
        {
            "name": "STaR4VQA predict(llama)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/lightning_train",
            "env": {
                "CUDA_VISIBLE_DEVICES":"6"
            },
            "justMyCode": false,
            "args": [
                "--stage", "predict", 
                "--batch_size", "16", 
                "--model_config", "llama-7b-qlora", 
                "--data_config", "instruction",
                "--ckpt_save_path", "/home/yadong/workspace/llama/STaR4VQA/output/bootstrap/epoch9/out", 
                "--predict_ckpt_name", "best.ckpt", 
                "--predict_output_dir", "/home/yadong/workspace/llama/STaR4VQA/output/bootstrap/v3.0", 
                "--predict_output_name", "dev-full_generated_vq.json",
                "--D.datamodule.args.data_dir=/home/yadong/workspace/llama/STaR4VQA/output/bootstrap/v3.0", 
                "--D.datamodule.args.predict_name=dev-full.json", 
                "--D.datamodule.args.predict_output_key=vq"
            ],
            "presentation": { "group": "STaR4VQA" }
        },
        {
            "name": "STaR4VQA predict(gpt2)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--stage","predict",
                "--batch_size","16",
                "--model_config", "gpt2",
                "--data_config", "instruction",
                "--num_beams","1",
                "--ckpt_save_path","/home/yadong/workspace/llama/STaR4VQA/output/bootstrap/epoch0/out",
                "--predict_ckpt_name","best.ckpt",
                "--predict_output_dir","/home/yadong/workspace/llama/STaR4VQA/output/bootstrap/epoch0",
                "--predict_output_name","generated_vq.json",
                "--D.datamodule.args.data_dir=/home/yadong/workspace/llama/STaR4VQA/output/bootstrap/epoch0",
                "--D.datamodule.args.train_name=instruction.json",
                "--D.datamodule.args.predict_name=../predict.json",
                "--D.datamodule.args.predict_output_key=vq"
            ],
            "presentation": { "group": "STaR4VQA" }
        },
        {
            "name": "OK-VQA train(gpt2)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"4"},
            "args": [
                "--stage","train",
                "--batch_size","16",
                "--model_config", "gpt2",
                "--data_config", "okvqa",
                "--num_beams","1",
                "--ckpt_save_path", "weights/okvqa",
                "--project", "OKVQA",
                "--run_name","general_caption (gpt2)"
            ],
            "presentation": {"group": "OK-VQA"}
        },
        {
            "name": "OK-VQA train(llama)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"7"},
            "args": [
                "--stage","train",
                "--batch_size","8",
                // "--warmup_rate", "0.1",
                "--model_config", "llama-7b-qlora",
                "--data_config", "okvqa",
                "--num_beams","1",
                "--ckpt_save_path", "weights/okvqa/caption_vqa",
                "--project", "OKVQA",
                "--run_name","llama_general-caption_refactor"
            ],
            "presentation": {"group": "OK-VQA"}
        },
        {
            "name": "OK-VQA test(llama)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "args": [
                "--stage","test",
                "--batch_size","32",
                "--model_config", "llama-7b-qlora",
                "--data_config", "okvqa",
                "--ckpt_save_path", "weights/okvqa/caption_vqa",
                // "--resume_ckpt", "base_vqa_acc=0.000_epoch=0.ckpt",
            ],
            "presentation": {"group": "OK-VQA"}
        },
        {
            "name": "OK-VQA predict(llama)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "args": [
                "--stage","predict",
                "--batch_size","16",
                "--model_config", "llama-7b-qlora",
                "--data_config", "okvqa",
                "--ckpt_save_path", "weights/okvqa",
                "--resume_ckpt", "base_vqa_acc=48.768_epoch=2.ckpt",
                "--D.datamodule.args.data_dir=/home/yadong/workspace/okvqa/datasets/dev-full",
                "--D.datamodule.args.caption_type=rl_2k_sample",
                "--predict_output_dir=output/okvqa/caption_vqa",
                "--predict_output_name=rl_2k_sample_predict.json"
            ],
            "presentation": {"group": "OK-VQA"}
        },
        {
            "name": "OK-VQA server",
            "type": "python",
            "request": "launch",
            "program": "server.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"5"},
            "args": [
                "--batch_size","16",
                "--model_config", "llama-7b-qlora",
                "--data_config", "okvqa",
                "--ckpt_save_path", "weights/okvqa",
                "--resume_ckpt", "base_vqa_acc=48.768_epoch=2.ckpt",
            ],
            "presentation": {"group": "OK-VQA"}
        },
        {
            "name": "AOK-VQA train(llama)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"4"},
            "args": [
                "--stage","train",
                "--batch_size","8",
                "--model_config", "llama-7b-qlora",
                "--data_config", "aokvqa",
                "--ckpt_save_path", "weights/aokvqa/caption_vqa",
                "--project", "AOKVQA",
                "--run_name","llama_ofa-caption"
            ],
            "presentation": {"group": "AOK-VQA"}
        },
        {
            "name": "AOK-VQA train(llama2)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"4"},
            "args": [
                "--stage","train",
                "--batch_size","8",
                "--model_config", "llama2-7b-qlora",
                "--data_config", "aokvqa",
                "--ckpt_save_path", "weights/aokvqa/caption_vqa",
                "--ckpt_name_prefix", "llama2",
                "--project", "AOKVQA",
                "--run_name","llama2_ofa-caption"
            ],
            "presentation": {"group": "AOK-VQA"}
        },
        {
            "name": "IMDB Positive train(gpt2)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"5"},
            "args": [
                "--stage","train",
                "--batch_size","256",
                "--model_config", "gpt2",
                "--data_config", "imdb_positive",
                "--ckpt_save_path", "weights/imdb",
                "--project", "IMDB-RL",
                "--run_name","positive_w_batch_gpt2",
                "--M.pretrain.path=/pretrains/huggingface/lvwerra/gpt2-imdb"
            ],
            "presentation": {"group": "IMDb"}
        },
        {
            "name": "IMDB Positive train(llama)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"5"},
            "args": [
                "--stage","train",
                "--batch_size","128",
                "--val_batch_size","16",
                "--mini_batch_size", "4",
                "--model_config", "llama-7b-qlora",
                "--data_config", "imdb_positive",
                "--ckpt_save_path", "weights/imdb_lm",
                "--project", "IMDB-RL",
                "--run_name","positive_w_batch_llama",
                "--M.pretrain.path=weights/imdb_lm/llama_lm_qlora_val_loss=2.543_epoch=0.ckpt.full"
            ],
            "presentation": {"group": "IMDb"}
        },
        {
            "name": "IMDB Positive test(gpt2)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"5"},
            "args": [
                "--stage","test",
                "--batch_size","256",
                "--model_config", "gpt2",
                "--data_config", "imdb_positive",
                "--ckpt_save_path", "weights/imdb",
                "--resume_ckpt", "base_val_reward=2.799_epoch=5.ckpt",
                "--M.pretrain.path=/pretrains/huggingface/lvwerra/gpt2-imdb"
            ],
            "presentation": {"group": "IMDb"}
        },
        {
            "name": "IMDB LM train(gpt2)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"0"},
            "args": [
                "--stage","train",
                "--batch_size","32",
                "--model_config", "gpt2",
                "--data_config", "imdb_lm",
                "--ckpt_save_path", "weights/imdb_lm",
                "--project", "IMDB-LM",
                "--run_name","gpt2_short",
                "--M.pretrain.path=/pretrains/huggingface/lvwerra/gpt2-imdb"
            ],
            "presentation": {"group": "IMDb"}
        },
        {
            "name": "IMDB LM train(llama)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "args": [
                "--stage","train",
                "--batch_size","4",
                "--max_epochs","6",
                "--model_config", "llama-7b-qlora",
                "--data_config", "imdb_lm",
                "--ckpt_save_path", "weights/imdb_lm",
                "--ckpt_name_prefix", "llama_lm_qlora",
                "--save_full_model",
                "--project", "IMDB-LM",
                "--run_name","llama_qlora_short"
            ],
            "presentation": {"group": "IMDb"}
        },
        {
            "name": "IMDB LM train(llama-bf16)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"5"},
            "args": [
                "--stage","train",
                "--batch_size","4",
                "--max_epochs","6",
                "--model_config", "llama-7b-bf16",
                "--data_config", "imdb_lm",
                "--ckpt_save_path", "weights/imdb_lm",
                "--ckpt_name_prefix", "llama_lm_bf16",
                "--save_full_model",
                "--project", "IMDB-LM",
                "--run_name","llama_bf16_short"
            ],
            "presentation": {"group": "IMDb"}
        },
        {
            "name": "RefCOCO train",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"7"},
            "args": [
                "--stage","train",
                "--batch_size","1",
                "--model_config", "llama-7b-qlora",
                "--data_config", "refcoco",
                "--ckpt_save_path", "weights/refcoco",
                "--project", "RefCOCO",
                "--run_name","sft_llama",
                "--D.datamodule.args.max_length=768"
            ],
            "presentation": {"group": "RefCOCO"}
        },
        {
            "name": "VQG Confident train(gpt2)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"5"},
            "args": [
                "--stage","train",
                "--batch_size","256",
                "--mini_batch_size","16",
                "--model_config", "gpt2",
                "--data_config", "vqg_vqa-score",
                "--ckpt_save_path", "weights/vqg",
                "--project", "VQG-Confident-RL",
                "--run_name","gpt2",
                "--val_batch_size=16",
                "--M.pretrain.path=weights/vqg/base_val_loss=0.641_epoch=0.ckpt.full"
            ],
            "presentation": {"group": "VQG-Confident-RL"}
        },
        {
            "name": "VQG Confident train(llama)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"4"},
            "args": [
                "--stage","train",
                "--batch_size","16",
                "--val_batch_size=8",
                "--mini_batch_size","2",
                "--accumulate_grads=8",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqg_confident_lr",
                "--ckpt_save_path", "weights/vqg",
                "--project", "VQG-Confident-RL",
                "--run_name","llama_lr1e4-ag8_ref_chatgpt-origin-question",
                "--D.ppo.generate_batch_size=8",
                "--M.pretrain.path=weights/vqg_lm/llama_lora8_chatgpt_val_loss=0.947_epoch=0.ckpt.full",
            ],
            "presentation": {"group": "VQG-Confident-RL"}
        },
        {
            "name": "VQG Confident train(llama-debug)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"7"},
            "args": [
                "--stage","train",
                "--batch_size","16",
                "--val_batch_size=8",
                "--mini_batch_size","2",
                "--accumulate_grads=8",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqg_confident_lr",
                "--ckpt_save_path", "weights/vqg",
                "--project", "VQG-RL",
                "--run_name","llama_lr1e4-ag8_ref_chatgpt-origin-question_helpful-reward",
                "--D.ppo.generate_batch_size=8",
                "--D.datamodule.train_name=def_full.json",
                "--M.pretrain.path=weights/vqg/llama_val_loss=0.138_epoch=2.ckpt.full",
            ],
            "presentation": {"group": "VQG-Confident-RL"}
        },
        {
            "name": "VQG Helpful train(gpt2)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "args": [
                "--stage","train",
                "--batch_size","256",
                "--mini_batch_size","8",
                "--model_config", "gpt2",
                "--data_config", "vqg_help-lr",
                "--ckpt_save_path", "weights/vqg",
                "--project", "lightning_train",
                "--run_name","gpt2_helpful_batch-llm",
                "--M.pretrain.path=weights/vqg/base_val_loss=0.641_epoch=0.ckpt.full"
            ],
            "presentation": {"group": "VQG-RL"}
        },
        {
            "name": "VQG Helpful train(gpt_chatglm)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"5"},
            "args": [
                "--stage","train",
                "--batch_size","16",
                "--mini_batch_size","8",
                "--model_config", "gpt2",
                "--data_config", "vqg_help-lr",
                "--ckpt_save_path", "weights/vqg",
                "--project", "VQG-RL",
                "--run_name","gpt2_chatglm_trainable",
                "--D.reward.args.llm=chatglm",
                "--M.pretrain.path=weights/vqg/base_val_loss=0.641_epoch=0.ckpt.full"
            ],
            "presentation": {"group": "VQG-RL"}
        },
        {
            "name": "VQG Helpful train(chatglm-inference)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"4"},
            "args": [
                "--stage","train",
                "--batch_size","16",
                "--mini_batch_size","4",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqg_help_lr",
                "--ckpt_save_path", "weights/vqg_help_lr",
                "--ckpt_name_prefix", "llama_chatglm",
                "--project", "VQG-RL",
                "--run_name","llama_chatglm-lemma_bs16_ref_vqg-lm",
                "--D.reward.args.llm=chatglm",
                "--M.pretrain.path=weights/vqg_lm/llama_lora8_chatgpt_val_loss=0.947_epoch=0.ckpt.full"
            ],
            "presentation": {"group": "VQG-Helpful-Train"}
        },
        {
            "name": "VQG Helpful train(chatglm-inference_bertscore)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "args": [
                "--stage","train",
                "--batch_size","16",
                "--mini_batch_size","2",
                "--accumulate_grads", "8",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqg_help_lr",
                "--ckpt_save_path", "weights/vqg_help_lr",
                "--ckpt_name_prefix", "llama_chatglm",
                "--project", "VQG-RL",
                "--run_name","llama_chatglm-lemma_bs16_ref_vqg-lm",
                "--D.reward.args.llm=chatglm",
                "--D.reward.args.scorer=bert",
                "--M.pretrain.path=weights/vqg_lm/llama_lora8_chatgpt_val_loss=0.947_epoch=0.ckpt.full"
            ],
            "presentation": {"group": "VQG-Helpful-Train"}
        },
        {
            "name": "VQG Helpful test(chatglm-inference)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "args": [
                "--stage","test",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqg_help-lr",
                "--ckpt_save_path", "weights/vqg",
                "--resume_ckpt", "sanity_vqa_acc=26.000_epoch=0.ckpt",
                "--D.reward.args.llm=chatglm",
                "--M.pretrain.path=weights/vqg/llama_chcked_val_loss=0.152_epoch=5.ckpt.full",
                "--M.optim.lr=1.41e-5"
            ],
            "presentation": {"group": "VQG-RL"}
        },
        {
            "name": "VQG Helpful train(llama2)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"7"},
            "args": [
                "--stage","train",
                "--batch_size","16",
                "--mini_batch_size","2",
                "--accumulate_grads=8",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqg_help_lr",
                "--ckpt_save_path", "weights/vqg_help_lr",
                "--ckpt_name_prefix", "llama_llama2",
                "--project", "VQG-RL",
                "--run_name","llama_llama2",
                "--D.datamodule.args.train_name=train_part_rl.json",
                "--D.reward.args.llm=llama",
                "--M.pretrain.path=weights/vqg_lm/llama-helpful-sft_val_loss=0.640_epoch=4.ckpt.full",
                "--no_early_stop", 

                // "--D.datamodule.args.train_name=train_1k_rl.json",
                // "--ckpt_name_prefix", "llama_llama2_1k",
                // "--max_epochs", "50",
                // "--no_early_stop", 
                //     "--run_name","llama_llama2_1k_no-cache",
                //         "--M.config.use_cache=false",
                //     // "--run_name","llama_llama2_1k_use-cache",
                
                "--D.reward.args.scorer=soft_vqa",
                "--D.datamodule.args.train_name=train_2k_rl.json",
                "--ckpt_name_prefix", "llama_llama2_2k_soft",
                "--max_epochs", "50",
                "--no_early_stop", 
                    "--run_name","llama_llama2_2k_no-cache_soft*2",
                        "--M.config.use_cache=false",
                    // "--run_name","llama_llama2_1k_use-cache",
                
                // "--M.pretrain.path=weights/vqg_lm/llama_lora32_val_loss=0.660_epoch=4.ckpt.full",
                // "--M.peft.r=32",
                // "--run_name","llama_llama2_2k_no-cache_r=32",
                // "--ckpt_name_prefix", "llama_llama2_2k_lora32",

            ],
            "presentation": {"group": "VQG-Helpful-Train"}
        },
        {
            "name": "VQG Helpful train(llama-inference_sanity)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "args": [
                "--stage","train",
                "--no_early_stop",
                "--batch_size","16",
                "--mini_batch_size","2",
                "--max_epochs", "100",
                "--accumulate_grads=8",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqg_help_lr",
                "--ckpt_save_path", "weights/vqg_help_lr",
                "--ckpt_name_prefix", "llama_llama2_sanity",
                "--project", "VQG-RL",
                "--run_name","llama_llama2-inference_sanity_nosample",
                "--D.datamodule.args.train_name=train_mini_rl.json",
                "--D.datamodule.args.val_name=train_mini_rl.json",
                "--D.reward.args.llm=llama",
                "--M.pretrain.path=weights/vqg_lm/llama-helpful-sft_val_loss=0.640_epoch=4.ckpt.full"
            ],
            "presentation": {"group": "VQG-Helpful-Train"}
        },
        {
            "name": "VQG Helpful train(llama-inference_sanity_no-cache)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"7"},
            "args": [
                "--stage","train",
                "--no_early_stop",
                "--batch_size","16",
                "--mini_batch_size","2",
                "--max_epochs", "100",
                "--accumulate_grads=8",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqg_help_lr",
                "--ckpt_save_path", "weights/vqg_help_lr",
                "--ckpt_name_prefix", "llama_llama2_sanity",
                "--project", "VQG-RL",
                "--run_name","llama_llama2-inference_sanity_nosample_nocache",
                "--D.datamodule.args.train_name=train_mini_rl.json",
                "--D.datamodule.args.val_name=train_mini_rl.json",
                "--D.reward.args.llm=llama",
                "--M.config.use_cache=false",
                "--M.pretrain.path=weights/vqg_lm/llama-helpful-sft_val_loss=0.640_epoch=4.ckpt.full"
            ],
            "presentation": {"group": "VQG-Helpful-Train"}
        },
        {
            "name": "VQG Helpful train(llama-inference_sanity_freeze)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"7"},
            "args": [
                "--stage","train",
                "--freeze",
                "--no_early_stop",
                "--batch_size","16",
                "--mini_batch_size","2",
                "--accumulate_grads=8",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqg_help_lr",
                "--ckpt_save_path", "weights/vqg_help_lr",
                "--ckpt_name_prefix", "llama_llama2_sanity",
                "--project", "VQG-RL",
                "--run_name","llama_llama2-inference_freeze_sanity_nosample",
                "--D.datamodule.args.train_name=train_mini_rl.json",
                "--D.datamodule.args.val_name=train_mini_rl.json",
                "--D.reward.args.llm=llama",
                "--M.pretrain.path=weights/vqg_lm/llama-helpful-sft_val_loss=0.640_epoch=4.ckpt.full"
            ],
            "presentation": {"group": "VQG-Helpful-Train"}
        },
        {
            "name": "VQG Helpful test(llama_chatgpt-inference)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"7"},
            "args": [
                "--stage","test",
                "--model_config", "llama-7b-qlora",
                "--batch_size","8",
                "--data_config", "vqg_help_lr",
                "--ckpt_save_path", "weights/vqg",
                "--D.datamodule.args.test_name=dev_full.json",
                "--D.reward.args.llm=chatgpt",
                "--D.reward.args.llm_batch_size=8",
                "--M.pretrain.path=weights/vqg/llama_lora8_val_loss=0.151_epoch=2.ckpt.full"
            ],
            "presentation": {"group": "VQG-RL"}
        },
        {
            "name": "VQG Helpful test(llama)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "args": [
                "--stage","test",
                "--batch_size","16",
                "--mini_batch_size","8",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqg_help-lr",
                "--D.datamodule.args.test_name=predict.json",
                "--ckpt_save_path", "weights/vqg",
                "--resume_ckpt", "base_vqa_acc=36.000_epoch=3.ckpt",
                "--project", "lightning_train",
                "--run_name","llama_helpful_batch-llm",
                "--M.pretrain.path=weights/vqg/llama_val_loss=0.138_epoch=2.ckpt.full"
            ],
            "presentation": {"group": "VQG-RL"}
        },
        {
            "name": "VQG Helpful test(gpt2)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "args": [
                "--stage","test",
                "--model_config", "gpt2",
                "--data_config", "vqg_help-lr",
                "--ckpt_save_path", "weights/vqg",
                "--resume_ckpt", "base_val_reward=0.160_epoch=9.ckpt",
                "--D.reward.args.llm_batch_size=16",
                "--D.datamodule.args.test_name=predict.json",
                "--M.pretrain.path=weights/vqg/base_val_loss=0.641_epoch=0.ckpt.full"
            ],
            "presentation": {"group": "VQG-RL"}
        },
        {
            "name": "VQG Helpful predict(gpt2)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "args": [
                "--stage","predict",
                "--model_config", "gpt2",
                "--data_config", "vqg_help-lr",
                "--ckpt_save_path", "weights/vqg",
                "--resume_ckpt", "base_val_reward=0.160_epoch=9.ckpt",
                "--D.reward.args.llm_batch_size=16",
                "--M.pretrain.path=weights/vqg/base_val_loss=0.641_epoch=0.ckpt.full"
            ],
            "presentation": {"group": "VQG-RL"}
        },
        {
            "name": "VQG Helpful predict(llama)",
            "type": "python",
            "request": "launch",
            "program": "rl_main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "args": [
                "--stage","predict",
                "--val_batch_size", "16",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqg_help_lr",
                "--ckpt_save_path", "weights/vqg_help_lr",
                "--predict_output_dir", "output/okvqa/vqg/helpful",
                "--D.datamodule.args.predict_name=../dev_full.json",
                "--D.reward.args.llm=llama",
                "--M.pretrain.path=weights/vqg_lm/llama-helpful-sft_val_loss=0.640_epoch=4.ckpt.full",

                "--M.config.use_cache=True",

                "--resume_ckpt","llama_llama2_2k_soft_vqa_acc=54.215_epoch=4.ckpt",
                "--predict_output_name", "rl_soft_vqa_acc=54.215_dev-full_generated_vq.json",
                
                // "--M.peft.r=32",
                
                // "--M.pretrain.path=weights/vqg_lm/llama_lora32_val_loss=0.660_epoch=4.ckpt.full",
                // "--resume_ckpt","llama_llama2_2k_lora32_vqa_acc=41.900_epoch=0.ckpt",
                // "--predict_output_name", "rl_2k_vqa_acc=41.900_sample_train-full_generated_vq.json",
                // "--D.datamodule.args.predict_name=../train_full.json",
            ],
            "presentation": {"group": "VQG-RL"}
        },
        {
            "name": "VQG alpaca train(gpt2)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"4"},
            "args": [
                "--stage","train",
                "--batch_size","64",
                "--model_config", "gpt2",
                "--data_config", "instruction",
                "--ckpt_save_path", "weights/vqg",
                "--project", "VQG-RL",
                "--run_name","sft_gpt2",
                "--D.datamodule.args.max_length=128",
                "--D.datamodule.args.train_name=instruction.json",
                "--D.datamodule.args.data_dir=/home/yadong/workspace/llama/lightning_train/data_dir/vqg"
            ],
            "presentation": {"group": "VQG-LM"}
        },
        {
            "name": "VQG alpaca train(gpt2)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"4"},
            "args": [
                "--stage","train",
                "--batch_size","64",
                "--model_config", "gpt2",
                "--data_config", "instruction",
                "--ckpt_save_path", "weights/vqg",
                "--project", "VQG-RL",
                "--run_name","sft_gpt2",
                "--D.datamodule.args.max_length=128",
                "--D.datamodule.args.train_name=instruction.json",
                "--D.datamodule.args.data_dir=/home/yadong/workspace/llama/lightning_train/data_dir/vqg"
            ],
            "presentation": {"group": "VQG-LM"}
        },
        {
            "name": "VQG train(gpt2)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"5"},
            "args": [
                "--stage","train",
                "--batch_size","32",
                "--model_config", "gpt2",
                "--data_config", "vqg",
                "--ckpt_save_path", "weights/vqg",
                "--save_full_model",
                // "--project", "VQG",
                // "--run_name","sft_gpt2",
            ],
            "presentation": {"group": "VQG-LM"}
        },
        {
            "name": "VQG LM train(llama)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"5"},
            "args": [
                "--stage","train",
                "--batch_size","16",
                "--max_epochs","10",
                "--disable_linear_schedule",
                "--log_every_n_steps", "1",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqg",
                "--ckpt_save_path", "weights/vqg_lm",
                "--save_full_model",
                "--ckpt_name_prefix", "llama_lora32",
                "--project", "VQG-LM",
                "--D.datamodule.args.data_dir=data_dir/okvqa/vqg/helpful",
                "--D.datamodule.args.train_name=train_part_sft.json",

                "--ckpt_name_prefix", "llama_lora32",
                "--run_name", "llama_lora-r=32",
                "--M.peft.r=32"

            ],
            "presentation": {"group": "VQG-LM"}
        },
        {
            "name": "VQG LM train(llama2)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"5"},
            "args": [
                "--stage","train",
                "--batch_size","16",
                "--max_epochs","10",
                "--disable_linear_schedule",
                "--log_every_n_steps", "1",
                "--model_config", "llama2-7b-qlora",
                "--data_config", "vqg",
                "--ckpt_save_path", "weights/vqg_lm",
                "--save_full_model",
                "--ckpt_name_prefix", "llama2",
                "--project", "VQG-LM",
                "--run_name", "llama2",
                "--D.datamodule.args.data_dir=data_dir/okvqa/vqg/helpful",
                "--D.datamodule.args.train_name=train_part_sft.json",
            ],
            "presentation": {"group": "VQG-LM"}
        },
        {
            "name": "VQG LM predict(llama)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "args": [
                "--stage","predict",
                "--batch_size","16",
                "--model_config", "llama-7b-qlora",
                "--data_config", "vqg",
                "--project", "VQG-LM",
                "--D.datamodule.args.data_dir=data_dir/okvqa/vqg",
                "--D.datamodule.args.predict_name=dev_full.json",
                

                // "--ckpt_save_path", "weights/vqg_lm",
                // "--resume_ckpt","llama_lora8_chatgpt_helpful_val_loss=0.640_epoch=4.ckpt",
                // "--predict_output_dir", "output/okvqa/vqg/lm",
                // "--predict_output_name", "sft_val_loss=0.640_dev-full_generated_vq.json",

                "--M.peft.r=32",
                "--ckpt_save_path", "weights/vqg_lm",
                "--resume_ckpt","llama_lora32_val_loss=0.660_epoch=4.ckpt",
                "--predict_output_dir", "output/okvqa/vqg/lm",
                "--predict_output_name", "sft_val_lora32_do-sample_loss=0.660_dev-full_generated_vq.json",
            ],
            "presentation": {"group": "VQG-LM"}
        },
        {
            "name": "DuIE train (chatglm)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"5"},
            "args": [
                "--stage","train",
                "--batch_size","1",
                "--val_batch_size","8",
                "--model_config", "chatglm2-6b-qlora",
                "--data_config", "duee-fin",
                "--ckpt_save_path", "weights/duie",
                "--ckpt_name_prefix", "chatglm",
                "--D.metrics.eval_calculate=false",
                "--project", "DuIE",
                "--run_name","chatglm2_use_cache"
            ],
            "presentation": {"group": "DuIE"}
        },
        {
            "name": "DuIE test (chatglm)",
            "type": "python",
            "request": "launch",
            "program": "main.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {"CUDA_VISIBLE_DEVICES":"6"},
            "args": [
                "--stage","test",
                "--batch_size","8",
                "--num_beams","1",
                "--model_config", "chatglm2-6b-qlora",
                "--data_config", "duee-fin",
                "--ckpt_save_path", "weights/duie",
                "--resume_ckpt","chatglm_val_loss=0.193_epoch=1.ckpt",
                "--D.datamodule.args.test_name=train_mini.json"
            ],
            "presentation": {"group": "DuIE"}
        }
    ]
}
