pretrain:
  path: /vault/pretrains/huggingface/huggyllama/llama-7b

config:
  use_cache: false
  pretraining_tp: 1 

quant:
  load_in_4bit:  
  bnb_4bit_quant_type: "nf4"

peft:
  lora_alpha: 16
  lora_dropout: 0.1
  r: 16
  bias: none

optim:
  lr: 1e-4