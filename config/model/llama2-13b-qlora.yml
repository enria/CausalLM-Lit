pretrain:
  path: /storage/pretrains/huggingface/meta-llama/Llama-2-13b-hf
  tokenizer_path: /storage/pretrains/huggingface/meta-llama/Llama-2-13b-hf
  args:
    trust_remote_code: true

tokenizer:
  manual_add_eos_token: true

config:
  use_cache: false
  do_sample: false

quant:
  bf16: true
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"

peft: 
  lora_alpha: 16
  lora_dropout: 0.05
  r: 8
  bias: none

optim:
  lr: 1e-4