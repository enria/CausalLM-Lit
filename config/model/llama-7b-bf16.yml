pretrain:
  path: /vault/pretrains/huggingface/huggyllama/llama-7b

config:
  use_cache: false
  pretraining_tp: 1 

precision:
  dtype: torch.bfloat16

peft:
  lora_alpha: 16
  lora_dropout: 0.1
  r: 16
  bias: none

optim:
  lr: 1e-4